{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "This homework will explore linear regression and resampling techniques by analysing data from a database of glaciers. The database is *Glatilda* for [*Glacier Ice Thickness Database*](!https://www.gtn-g.ch/data_catalogue_glathida/).\n",
    "\n",
    "1. Data prep (5 points)\n",
    "2. Mapping (10 points)\n",
    "3. Correlations between parameters (5 points)\n",
    "4. Linear regression and resampling techniques (10 points)\n",
    "\n",
    "## 1. Data Prep (5 points total)\n",
    "\n",
    "### a) Download data (1 point) \n",
    "The database is saved on a GitLab repository that you may clone: https://gitlab.com/wgms/glathida.git\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://gitlab.com/wgms/glathida.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = 'glathida/data/band.csv'\n",
    "glacier = 'glathida/data/glacier.csv'\n",
    "point = 'glathida/data/point.csv'\n",
    "survey = 'glathida/data/survey.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Import Python modules (1 point) \n",
    "Import pandas, geopandas, plotting, raster files,  numpy, netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wget\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import wget\n",
    "import requests, zipfile , os, io\n",
    "\n",
    "from rasterio.mask import mask\n",
    "from rasterio.plot import show\n",
    "\n",
    " \n",
    "import plotly.express as px\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Read data (2 points)\n",
    "Read the glacier data from the file ``glathida/data/glacier.csv`` into a pandas data frame, and decribe briefly the dataframe content and its first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data set has (1013, 20) rows and columns\n",
      "         survey_id          lat          lon          area  mean_slope  \\\n",
      "count  1013.000000  1013.000000  1013.000000    985.000000  426.000000   \n",
      "mean    130.707799    57.084700    14.987602    217.599979    9.091549   \n",
      "std      75.321429    29.473871    51.341702   1602.939314    5.806169   \n",
      "min       1.000000   -74.583300  -151.300000      0.026400    0.000000   \n",
      "25%      69.000000    46.453610    10.700000      2.204000    6.000000   \n",
      "50%     128.000000    62.039400    14.687100     10.959000    8.000000   \n",
      "75%     203.000000    78.776000    22.280000     98.341000   12.000000   \n",
      "max     256.000000    81.767200   170.320000  40000.000000   48.000000   \n",
      "\n",
      "       mean_thickness  mean_thickness_uncertainty  max_thickness  \\\n",
      "count      498.000000                  127.000000     525.000000   \n",
      "mean        70.375502                    7.125984     197.563810   \n",
      "std         69.053311                    5.988073     199.968677   \n",
      "min          4.000000                    0.000000      12.000000   \n",
      "25%         33.250000                    3.000000      95.000000   \n",
      "50%         51.000000                    5.000000     140.000000   \n",
      "75%         74.000000                    9.000000     230.000000   \n",
      "max        541.000000                   30.000000    2500.000000   \n",
      "\n",
      "       max_thickness_uncertainty  number_points  number_profiles  \\\n",
      "count                 135.000000     593.000000        68.000000   \n",
      "mean                   10.074074    3086.131535        15.779412   \n",
      "std                     6.977180    8117.336773        17.335018   \n",
      "min                     1.000000       1.000000         1.000000   \n",
      "25%                     5.000000      46.000000         6.000000   \n",
      "50%                     8.000000     189.000000        10.500000   \n",
      "75%                    13.000000    2018.000000        21.000000   \n",
      "max                    36.000000   67542.000000       120.000000   \n",
      "\n",
      "       length_profiles  \n",
      "count       141.000000  \n",
      "mean         48.587333  \n",
      "std         183.175981  \n",
      "min           0.140000  \n",
      "25%           3.000000  \n",
      "50%           9.500000  \n",
      "75%          22.500000  \n",
      "max        1657.000000  \n"
     ]
    }
   ],
   "source": [
    "# solution\n",
    "t_path = \"glathida/data/glacier.csv\"\n",
    "glacier = pd.read_csv(t_path, index_col=0)\n",
    "\n",
    "print(\"This data set has\", glacier.shape, \"rows and columns\" )\n",
    "print(glacier.describe())\n",
    "#There are many columns that contain rows with NaN values\n",
    "#This dataset contains location, data, area, ID, thickness parameters, mean slop data\n",
    "#It also contains additional important information pertating to the profiles that were conducted, and methods used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survey_id</th>\n",
       "      <th>name</th>\n",
       "      <th>external_db</th>\n",
       "      <th>external_id</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>date</th>\n",
       "      <th>max_date</th>\n",
       "      <th>area</th>\n",
       "      <th>mean_slope</th>\n",
       "      <th>mean_thickness</th>\n",
       "      <th>mean_thickness_uncertainty</th>\n",
       "      <th>max_thickness</th>\n",
       "      <th>max_thickness_uncertainty</th>\n",
       "      <th>number_points</th>\n",
       "      <th>number_profiles</th>\n",
       "      <th>length_profiles</th>\n",
       "      <th>interpolation_method</th>\n",
       "      <th>flag</th>\n",
       "      <th>remarks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Isfallsglaciären</td>\n",
       "      <td>WGI</td>\n",
       "      <td>SE4B000E0006</td>\n",
       "      <td>67.91500</td>\n",
       "      <td>18.56800</td>\n",
       "      <td>1979-03-01</td>\n",
       "      <td>1979-03-31</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>72.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>220.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Rabots glaciär</td>\n",
       "      <td>WGI</td>\n",
       "      <td>SE4B000E1016</td>\n",
       "      <td>67.91000</td>\n",
       "      <td>18.49600</td>\n",
       "      <td>1979-03-01</td>\n",
       "      <td>1979-03-31</td>\n",
       "      <td>4.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>175.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Storglaciären</td>\n",
       "      <td>WGI</td>\n",
       "      <td>SE4B000E0005</td>\n",
       "      <td>67.90000</td>\n",
       "      <td>18.57000</td>\n",
       "      <td>1979-03-01</td>\n",
       "      <td>1979-03-31</td>\n",
       "      <td>3.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>South Cascade Glacier</td>\n",
       "      <td>WGI</td>\n",
       "      <td>US2M00264006</td>\n",
       "      <td>48.35698</td>\n",
       "      <td>-121.05735</td>\n",
       "      <td>1975-01-01</td>\n",
       "      <td>1975-12-31</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>195.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>Athabasca Glacier</td>\n",
       "      <td>FOG</td>\n",
       "      <td>7</td>\n",
       "      <td>52.17540</td>\n",
       "      <td>-117.28400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    survey_id                   name external_db   external_id       lat  \\\n",
       "id                                                                         \n",
       "1           1       Isfallsglaciären         WGI  SE4B000E0006  67.91500   \n",
       "2           1         Rabots glaciär         WGI  SE4B000E1016  67.91000   \n",
       "3           1          Storglaciären         WGI  SE4B000E0005  67.90000   \n",
       "4           2  South Cascade Glacier         WGI  US2M00264006  48.35698   \n",
       "5           3      Athabasca Glacier         FOG             7  52.17540   \n",
       "\n",
       "          lon        date    max_date  area  mean_slope  mean_thickness  \\\n",
       "id                                                                        \n",
       "1    18.56800  1979-03-01  1979-03-31   1.3         NaN            72.0   \n",
       "2    18.49600  1979-03-01  1979-03-31   4.1         NaN            84.0   \n",
       "3    18.57000  1979-03-01  1979-03-31   3.1         NaN            99.0   \n",
       "4  -121.05735  1975-01-01  1975-12-31   2.0         NaN            99.0   \n",
       "5  -117.28400         NaN         NaN   3.8         NaN           150.0   \n",
       "\n",
       "    mean_thickness_uncertainty  max_thickness  max_thickness_uncertainty  \\\n",
       "id                                                                         \n",
       "1                          NaN          220.0                        NaN   \n",
       "2                          NaN          175.0                        NaN   \n",
       "3                          NaN          250.0                        NaN   \n",
       "4                          NaN          195.0                        NaN   \n",
       "5                          NaN            NaN                        NaN   \n",
       "\n",
       "    number_points  number_profiles  length_profiles interpolation_method flag  \\\n",
       "id                                                                              \n",
       "1             NaN              NaN              NaN                  NaN  NaN   \n",
       "2             NaN             10.0              NaN                  NaN  NaN   \n",
       "3             NaN              NaN              NaN                  NaN  NaN   \n",
       "4             NaN              NaN              NaN                  NaN  NaN   \n",
       "5             NaN              NaN              NaN                  NaN  NaN   \n",
       "\n",
       "   remarks  \n",
       "id          \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  \n",
       "5      NaN  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glacier.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explore the data with visualization**\n",
    "Before making any inference of models with the data, we will start by exploring basic correlations among parameters by plotting. In particular, we will focus on ``mean_thickness``, ``area``, ``mean_slope`` parameters.\n",
    "\n",
    "### d) Remove bad data (1 point)\n",
    "\n",
    "The database may contain Nans and other \"bad\" values (welcome to the data world!). First we will clean the data by removing nans. We are mostly interested in the thickness, area, and slope\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer below \n",
    "glacier.dropna(subset=['mean_thickness', 'area', 'mean_slope'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mapping glaciers (10 points)\n",
    "\n",
    "Make a global map of the glaciers. Use either of the tools we learned in class:\n",
    "* Geopandas, DEMs from NetCDFfiles (see chapter 2.4)\n",
    "* Pandas and Plotly (see chapter 2.2). You may need to transform some of the series into log-spaced values for better visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Tif and matplotlib\n",
    "\n",
    "You can use the ``elevation`` data from the DEM seen in class. Download the DEM file (https://www.dropbox.com/s/j5lxhd8uxrtsxko/HYP_50M_SR.tif?dl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calls for and stores the data for use in this notebook\n",
    "elevation = rasterio.open(\"https://www.dropbox.com/s/j5lxhd8uxrtsxko/HYP_50M_SR.tif?dl=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "___Tips___: when plotting a image in ``matplotlib`` you need to add information about the physical dimensions of the image. You can calculate the ``bounds``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = (elevation.bounds.left, elevation.bounds.right, \\\n",
    "          elevation.bounds.bottom, elevation.bounds.top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<open DatasetReader name='https://www.dropbox.com/s/j5lxhd8uxrtsxko/HYP_50M_SR.tif?dl=1' mode='r'>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use ``matplotlib.pyplot`` to show the raster image in the background (tips: use ``imshow()``. The raster image in matplotlib can only import one frame and not three (R, G, B) frames. We will first stack the three images together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: TIFFReadEncodedStrip:Read error at scanline 4294967295; got 0 bytes, expected 32400\n",
      "ERROR 1: TIFFReadEncodedStrip() failed.\n",
      "ERROR 1: /vsicurl/https://www.dropbox.com/s/j5lxhd8uxrtsxko/HYP_50M_SR.tif?dl=1, band 1: IReadBlock failed at X offset 0, Y offset 4805: TIFFReadEncodedStrip() failed.\n"
     ]
    },
    {
     "ename": "RasterioIOError",
     "evalue": "Read or write failed. /vsicurl/https://www.dropbox.com/s/j5lxhd8uxrtsxko/HYP_50M_SR.tif?dl=1, band 1: IReadBlock failed at X offset 0, Y offset 4805: TIFFReadEncodedStrip() failed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_AppDefinedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mrasterio/_io.pyx:936\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase._read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_io.pyx:174\u001b[0m, in \u001b[0;36mrasterio._io.io_multi_band\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_err.pyx:191\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_int\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_AppDefinedError\u001b[0m: /vsicurl/https://www.dropbox.com/s/j5lxhd8uxrtsxko/HYP_50M_SR.tif?dl=1, band 1: IReadBlock failed at X offset 0, Y offset 4805: TIFFReadEncodedStrip() failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [95], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m red \u001b[38;5;241m=\u001b[39m elevation\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      2\u001b[0m green \u001b[38;5;241m=\u001b[39m elevation\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m blue \u001b[38;5;241m=\u001b[39m elevation\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32mrasterio/_io.pyx:612\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase.read\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_io.pyx:939\u001b[0m, in \u001b[0;36mrasterio._io.DatasetReaderBase._read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: Read or write failed. /vsicurl/https://www.dropbox.com/s/j5lxhd8uxrtsxko/HYP_50M_SR.tif?dl=1, band 1: IReadBlock failed at X offset 0, Y offset 4805: TIFFReadEncodedStrip() failed."
     ]
    }
   ],
   "source": [
    "red = elevation.read(1)\n",
    "green = elevation.read(2)\n",
    "blue = elevation.read(3)\n",
    "pix = np.dstack((red, green, blue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pix, cmap=plt.cm.viridis,\n",
    "                 extent=bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Plotly\n",
    "\n",
    "You may use plotly. For improved visibility, transform some of the data into log-spaced. You may add these transformed Series into the Pandas, and use them as input to plotly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'vscode' # writes as standalone html, \n",
    "# pio.renderers.default = 'iframe' # writes files as standalone html, \n",
    "# pio.renderers.default = 'png' # writes files as standalone html, \n",
    "# try notebook, jupyterlab, png, vscode, iframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlations between data parameters ( 5 points total)\n",
    "\n",
    "Make plots to vizualise the correlation, or lack of, between all three data. Make at least three plots.\n",
    "\n",
    "### a) Basic correlations using Matplotlib (2 points)\n",
    "\n",
    "Make 3 plots using matplotlib to visualize slope, mean_thickness, and area. Use logscale to see the correlatons.\n",
    "\n",
    "__Tips__: \n",
    "* Use the function ``scatter`` to plot the values of mean thickness, mean slope, area, and latitude. \n",
    "* use one of the dataframe columns as a color using the argument ``c``. You can also vary the ``colormap`` using the argument ``cmap``. Help on colormaps can be found here: https://matplotlib.org/stable/tutorials/colors/colormaps.html. Be mindful of Color-Vision Deficient readers and read *Crameri, F., Shephard, G.E. and Heron, P.J., 2020. The misuse of colour in science communication. Nature communications, 11(1), pp.1-10. https://doi.org/10.1038/s41467-020-19160-7* (find it on the class Gdrive). You can add a third \"data\" by choosing a marker color that scales with an other parameter. For instance, try coloring your marker with the ``LAT`` parameter to look at systematic latitudinal trends from the equator to the poles.\n",
    "* Do not forget to adjust fontsize, figure size (at least 10,8), grid, labels with  of the features (example: km). ou may also explore the *logarithmic* correlations by mapping the axis from linear to logarithmic scale ``plt.xscale('log')``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Mean slope vs mean thickness\n",
    "# solution\n",
    "fig, axs = plt.subplots(figsize=(10,8))\n",
    "\n",
    "slope_thickness = axs.scatter(data=glacier, x='mean_slope', y='mean_thickness', c='lat', cmap='Blues')\n",
    "axs.set_xlabel('Mean Slope (º)', fontsize=12); axs.set_ylabel('Mean Thickness (m)', fontsize=12)\n",
    "\n",
    "axs.set_ylim(0,125); #axs.set_xlim(0,100)\n",
    "#easier to see correlation with log scale on x-axis\n",
    "\n",
    "plt.colorbar(slope_thickness, label= 'Latitude (º)', orientation='horizontal')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.tight_layout(pad=1)\n",
    "fig.suptitle('Preliminary Correlation Analysis of Galcier Parameters Mean Thickness and Mean Slope'\n",
    "             , y=1.02, fontsize=15) # or plt.suptitle('Main title')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: area vs mean thickness\n",
    "# solution\n",
    "fig, axs = plt.subplots(figsize=(10,8))\n",
    "\n",
    "area_thickness = axs.scatter(data=glacier, x='area', y='mean_thickness',  c='lat', cmap='Blues')\n",
    "\n",
    "axs.set_ylabel('Glacier Area (km^2)', fontsize=12); axs.set_xlabel('Mean Thickness (m)', fontsize=12)\n",
    "\n",
    "plt.colorbar(area_thickness, label= 'Latitude (º)', orientation='horizontal')\n",
    "#easier to see correlation with log scale on x-axis\n",
    "plt.xscale('log')\n",
    "plt.tight_layout(pad=1)\n",
    "fig.suptitle('Preliminary Correlation Analysis of Galcier Parameters Area and Mean Thickness',\n",
    "             y=1.01, fontsize=15) # or plt.suptitle('Main title')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: area vs mean slope\n",
    "# solution\n",
    "fig, axs = plt.subplots(figsize=(10,8))\n",
    "\n",
    "area_slope = axs.scatter(data=glacier, x='area', y='mean_slope',  c='lat', cmap='Blues')\n",
    "axs.set_ylabel('Mean Slope (º)', fontsize=12); axs.set_xlabel('Glacier Area (km^2)', fontsize=12)\n",
    "\n",
    "plt.colorbar(area_slope, label= 'Latitude (º)', orientation='horizontal')\n",
    "#easier to see correlation with log scale on x-axis\n",
    "plt.xscale('log')\n",
    "plt.tight_layout(pad=1)\n",
    "fig.suptitle('Preliminary Correlation Analysis of Galcier Parameters Area and Mean Slope',\n",
    "             y=1.01, fontsize=15) # or plt.suptitle('Main title')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) 3D Scatter plot using Plotly (1 point)\n",
    "\n",
    "Use the plotly ``scatter_3d`` plot. Make sure to change the pandas series for log scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "px.scatter_3d(glacier, x='area', y='mean_slope', z='mean_thickness',\n",
    "              color='lat', log_x=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Pandas Correlation function (1 point)\n",
    "\n",
    "You may use Pandas functionalities to explore correlation between data. Use the function ``corr`` on the dataframe and the matplotlib function ``matshow`` to plot a heatmap of the correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting which parameters I want to look at\n",
    "glacier_parameters = glacier[['mean_thickness','mean_slope','area']]\n",
    "\n",
    "#getting the correlations of all these parameters\n",
    "correlations = glacier_parameters.corr()\n",
    "\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating figure\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "#ax.set_xticks(np.arange(len(labels)))  # show them all!\n",
    "#ax.set_yticks(np.arange(len(labels)))  # show them all!\n",
    "ax.set_xticklabels(['0','Mean Thickness','Mean Slope','Area'])  # set to be the abbv (vs useless #)\n",
    "ax.set_yticklabels(['0','Mean Thickness','Mean Slope','Area'])\n",
    "\n",
    "correlation_plot = ax.matshow(correlations, cmap='Blues')\n",
    "\n",
    "plt.colorbar(correlation_plot, label= 'Correlation', orientation='vertical')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Seaborn Plotting (1 point)\n",
    "\n",
    "Seaborn is a great python package for basic data anlytics. See documentation [here](!https://seaborn.pydata.org/). You can visualize the data by plotting data features against each other and explore visually data correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3,1,figsize=(10,8))\n",
    "\n",
    "sns.scatterplot(data=glacier, x='mean_thickness', y='mean_slope', ax=ax[0])\n",
    "\n",
    "sns.scatterplot(data=glacier, x='area', y='mean_thickness', ax=ax[1])\n",
    "\n",
    "sns.scatterplot(data=glacier, x='area', y='mean_slope', ax=ax[2])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,3):\n",
    "    ax[i].set(xscale=\"log\")\n",
    "\n",
    "plt.tight_layout(pad=1.05)\n",
    "plt.suptitle('Preliminary Correlation Analysis with Seaborn on Glacier Parameters', y=1.01)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the basic correlations among the data. Do these correction make sense when you think about the shapes of glaciers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean thickness and mean slope have a moderate negative correlation with a value of -0.412, this makes sence as a thicker iceberg is generally more flat than a smaller iceberg when just thinking of the enviorment. \n",
    "\n",
    "Area and mean thickness have a moderate postive correlation of 0.42, a number similar, though postive, to mean slope and mean thickness. When thinking of a glacial enviorment it makes sence that a glacier with a larger area is thicker as glaciers when larger are generally thick as well.\n",
    "\n",
    "Area and mean slope have the least strong correlation of the three parameters against eachother at with a correlation value of -0.103678, making it not a significant correlation. This doesn't seem at the of the ordinary when you take into account how glacier can be large but very flat, or have many slopes, likely due to the enviorment they developed and sustained in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Linear Regression (10 points total counted in the next section)\n",
    "You found from basic data visualization that the three parameters ``mean_slope``, ``mean_thickness``, and ``area`` are correlated. It does make physical sense because a *steep* glaciers is likely to be in the high mountains regions, hanging on the mountain walls, and thus be constrained, and conversely, a flat glacier is either at its valley, ocean terminus or on ice sheets.\n",
    "\n",
    "### a) Simple linear regression (2 points)\n",
    "We will now perform a regression between the parameters (or their log!). Linear regressions are models that can be imported from scikit-learn. Log/exp functions in numpy as ``np.log()`` and ``np.exp()``.\n",
    "Remember that a linear regression is finding $a$ and $b$ knowing both $x$ and the data $y$ in $y = Ax +b$. We want to predict ice thickness from a crude estimate of the glacier area.\n",
    "\n",
    "__Tips__: \n",
    "a. make sure that the dimensions are correct and that there is no NaNs and zeros.\n",
    "b. Make sure to inport the scikit learn linear regression function and the error metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#any nans or zeros?\n",
    "area = glacier['area']\n",
    "slope = glacier['mean_slope']\n",
    "thick = glacier['mean_thickness']\n",
    "\n",
    "print('Area:', len(glacier[area.isna()]), len(glacier[area==0]),\n",
    "      'Thickness:', len(glacier[thick.isna()]), len(glacier[thick==0]),\n",
    "      'Slope:', len(glacier[slope.isna()]), len(glacier[slope==0])\n",
    "                                             )\n",
    "# remove zeros from slope\n",
    "glacier = glacier[(glacier.mean_slope != 0)]\n",
    "\n",
    "print(len(glacier[slope==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a plot of the data and the linear regression your performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## thickness and mean slope\n",
    "# converting the data into numpy arrays\n",
    "y = np.asarray(glacier['mean_thickness']).reshape(-1, 1)# reshaping so that it works for linear regress as it requries it\n",
    "area_log = np.log(glacier['area']) #putting area into log space\n",
    "x = np.asarray(area_log).reshape(-1, 1) #reshaping area area\n",
    "#tt = np.linspace(np.min(t),np.max(t),1000)\n",
    "\n",
    "# performing the linear regression\n",
    "regr = LinearRegression()\n",
    "# fitting it\n",
    "regr.fit(x,y)\n",
    "# We will first predict the fit:\n",
    "prediction=regr.predict(x) \n",
    "\n",
    "# The coefficients\n",
    "print('The coefficent is', regr.coef_[0][0],\n",
    "      'and intercept is', regr.intercept_)\n",
    "\n",
    "plt.plot(x,prediction,color=\"red\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 2: area vs mean thickness\n",
    "# solution\n",
    "fig, axs = plt.subplots(figsize=(10,8))\n",
    "area_log = np.log(glacier['area'])\n",
    "area_thickness = axs.scatter(data=glacier, x=area_log, y='mean_thickness',  c='lat', cmap='Blues')\n",
    "\n",
    "axs.set_xlabel('Glacier Area (km^2)', fontsize=12); axs.set_ylabel('Mean Thickness (m)', fontsize=12)\n",
    "\n",
    "plt.colorbar(area_thickness, label= 'Latitude (º)', orientation='horizontal')\n",
    "#easier to see correlation with log scale on x-axis\n",
    "\n",
    "# x is area\n",
    "plt.plot(x,prediction,color=\"red\")\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=1)\n",
    "fig.suptitle('Correlation Analysis of Galcier Parameters Area and Mean Thickness',\n",
    "             y=1.01, fontsize=15) # or plt.suptitle('Main title')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Briefly comment on the quality of your fit and a linear regression (1 point)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit looks pretty good, except that there are outliers on the extreme low and high values of mean area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Leave One Out Cross Validation linear regression (1 point)\n",
    "\n",
    "\n",
    "Perform the LOCCV on the ``area`` and ``thickness`` values. Predict the ``thickness`` value knowing a ``area`` value. Use material seen in class. Make a plot of your fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# solution\n",
    "loo = LeaveOneOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data into numpy arrays.\n",
    "E = np.asarray(glacier['mean_thickness']).reshape(-1, 1)# reshaping was necessary to be an argument of Linear regress\n",
    "t = np.asarray(glacier['area']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vel = np.zeros(len(E)) # initalize a vector to store the regression values\n",
    "mse_train = np.zeros(len(E))\n",
    "mse_val = np.zeros(len(E))\n",
    "r2s = np.zeros(len(E))\n",
    "i=0\n",
    "for train_index, test_index in loo.split(E):    \n",
    "    E_train, E_val = E[train_index], E[val_index]\n",
    "    t_train, t_val = t[train_index], t[val_index]\n",
    "    # now fit the data on the training set.\n",
    "    regr = LinearRegression()\n",
    "    # Fit on training data:\n",
    "    regr.fit(t_train,E_train)\n",
    "    # We will first predict the fit:\n",
    "    Epred_train=regr.predict(t_train) \n",
    "    Epred_val=regr.predict(t_val) \n",
    "\n",
    "    # The coefficients\n",
    "    vel[i]= regr.coef_[0][0]\n",
    "    mse_train[i]= mean_squared_error(E_train, Epred_train)\n",
    "    mse_val[i]= mean_squared_error(E_val, Epred_val)\n",
    "    r2s[i]=r2_score(E_val, Epred_val)\n",
    "    i+=1\n",
    "\n",
    "# the data shows cleary a trend, so the predictions of the trends are close to each other:\n",
    "print(\"mean estimate is %f4.2 and the standard deviation %f4.2\"%(np.mean(vel),np.std(vel)))\n",
    "# the test error is the average of the mean-square-errors\n",
    "print(\"CV = %4.2f\"%(np.mean(mse_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we randomly select values and split the data between training and validation set.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "# we split once the data between a training and a validating set \n",
    "n=1 # we do this selectio once\n",
    "v_size = 0.3 # 30% of the data will be randomly selected to be the validation set.\n",
    "\n",
    "rs = ShuffleSplit(n_splits=n, test_size=.3, random_state=0)\n",
    "for train_index, val_index in rs.split(E):\n",
    "    E_train, E_val = E[train_index], E[val_index]\n",
    "    t_train, t_val = t[train_index], t[val_index]\n",
    "plt.scatter(t_train,E_train,marker=\"o\");plt.grid(True);plt.ylabel('Thickness')\n",
    "plt.scatter(t_val,E_val,marker=\"o\",s=6,c=\"red\")\n",
    "plt.xlabel('Area (km^2)')\n",
    "plt.title('Thickness Predictions using Area')\n",
    "plt.legend(['training set','validation set'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit the data on the training set.\n",
    "regr = LinearRegression()\n",
    "# Fit on training data:\n",
    "regr.fit(t_train,E_train)\n",
    "# We will first predict the fit:\n",
    "Epred=regr.predict(t_train) \n",
    "Epred_val=regr.predict(t_val) \n",
    "\n",
    "# The coefficients\n",
    "print('Training set: Coefficient (m): ', regr.coef_[0][0])\n",
    "\n",
    "print('MSE (mean square error) on training set (m): %.2f'\n",
    "      % mean_squared_error(Epred, E_train))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination on training set: %.2f'\n",
    "      % r2_score(Epred, E_train))\n",
    "\n",
    "print('MSE on validation set (m): %.2f and coefficient of determiniation on %.2f' %(mean_squared_error(Epred_val, E_val), r2_score(Epred_val, E_val)))\n",
    "\n",
    "\n",
    "plt.scatter(t,E);plt.grid(True);plt.ylabel('Glacier Thickness')\n",
    "plt.plot(t_train,Epred,color=\"red\",linewidth=4)\n",
    "plt.plot(t_val,Epred_val,color=\"green\")\n",
    "plt.legend(['data','fit on training','fit on validation'])\n",
    "plt.title('Random selection for data split')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Bootstrapping (1 point)\n",
    "\n",
    "Perform the same analysis but using a bootstrapping technique. Output the mean and standard deviation of the slope. An illustration with a histogram  may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "# solution\n",
    "\n",
    "# convert the data into numpy arrays.\n",
    "E = np.asarray(glacier['mean_thickness']).reshape(-1, 1)# reshaping was necessary to be an argument of Linear regress\n",
    "t = np.asarray(area_log).reshape(-1, 1) #TRANSFORM THIS INTO LOG SPACE\n",
    "\n",
    "k=100\n",
    "\n",
    "vel = np.zeros(k) # initalize a vector to store the regression values\n",
    "mse = np.zeros(k)\n",
    "r2s = np.zeros(k)\n",
    "i=0\n",
    "for iik in range(k):    \n",
    "    ii = resample(np.arange(len(E)),replace=True,n_samples=len(E))# new indices\n",
    "    E_b, t_b = E[ii], t[ii]\n",
    "    # now fit the data on the training set.\n",
    "    regr = LinearRegression()\n",
    "    # Fit on training data:\n",
    "    regr.fit(t_b,E_b)\n",
    "    Epred_val=regr.predict(t) # test on the validation set.\n",
    "\n",
    "    # The coefficients\n",
    "    vel[i]= regr.coef_[0][0]\n",
    "    i+=1\n",
    "\n",
    "# the data shows cleary a trend, so the predictions of the trends are close to each other:\n",
    "print(\"mean thickness estimates %f4.2 and the standard deviation %f4.2\"%(np.mean(vel),np.std(vel)))\n",
    "\n",
    "plt.hist(vel,50);plt.title('Distribution of Thickness');plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Predict the thickness of a glacier (2 points)\n",
    "\n",
    "Let assume that you measure a glacier of area 10 km$^2$. Can you use your bootstrap regression framework to provide a distribution of possible values of the ice thickness ? Output the mean and standard deviation of the predicted ice thickness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "k=100\n",
    "t = np.asarray(area_log).reshape(-1, 1)\n",
    "vel = np.zeros(k) # initalize a vector to store the regression values\n",
    "mse = np.zeros(k)\n",
    "r2s = np.zeros(k)\n",
    "i=0\n",
    "for iik in range(k):    \n",
    "    ii = resample(np.arange(len(E)),replace=True,n_samples=len(E))# new indices\n",
    "    E_b, t_b = E[ii], t[ii]\n",
    "    # now fit the data on the training set.\n",
    "    regr = LinearRegression()\n",
    "    # Fit on training data:\n",
    "    regr.fit(t_b,E_b)\n",
    "    Epred_val=regr.predict(t) # test on the validation set.\n",
    "\n",
    "    # The coefficients\n",
    "    vel[i]= np.exp(regr.predict(np.log(10)*np.ones(1).reshape(-1,1)))\n",
    "    i+=1\n",
    "\n",
    "print('mean:',np.mean(vel),'STD:',np.std(vel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c2df93b363d800c8a9b94963221f1be1d8deaf6a76f83b6b9a486ad05d69583"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
